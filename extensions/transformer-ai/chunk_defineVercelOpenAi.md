#### Open AI Provider Types

extensions/transformer-ai/node_modules/@ai-sdk/openai/dist/index.d.ts

```typescript
import { LanguageModelV1 } from "@ai-sdk/provider";

type OpenAIChatModelId =
	| "gpt-4-turbo"
	| "gpt-4-turbo-2024-04-09"
	| "gpt-4-turbo-preview"
	| "gpt-4-0125-preview"
	| "gpt-4-1106-preview"
	| "gpt-4-vision-preview"
	| "gpt-4"
	| "gpt-4-0613"
	| "gpt-4-32k"
	| "gpt-4-32k-0613"
	| "gpt-3.5-turbo-0125"
	| "gpt-3.5-turbo"
	| "gpt-3.5-turbo-1106"
	| "gpt-3.5-turbo-16k"
	| "gpt-3.5-turbo-0613"
	| "gpt-3.5-turbo-16k-0613"
	| (string & {});
interface OpenAIChatSettings {
	/**
	 * Modify the likelihood of specified tokens appearing in the completion.
	 *
	 * Accepts a JSON object that maps tokens (specified by their token ID in
	 * the GPT tokenizer) to an associated bias value from -100 to 100. You
	 * can use this tokenizer tool to convert text to token IDs. Mathematically,
	 * the bias is added to the logits generated by the model prior to sampling.
	 * The exact effect will vary per model, but values between -1 and 1 should
	 * decrease or increase likelihood of selection; values like -100 or 100
	 * should result in a ban or exclusive selection of the relevant token.
	 *
	 * As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
	 * token from being generated.
	 */
	logitBias?: Record<number, number>;
	/**
	 * A unique identifier representing your end-user, which can help OpenAI to
	 * monitor and detect abuse. Learn more.
	 */
	user?: string;
}

type OpenAIChatConfig = {
	provider: string;
	baseUrl: string;
	headers: () => Record<string, string | undefined>;
};
declare class OpenAIChatLanguageModel implements LanguageModelV1 {
	readonly specificationVersion = "v1";
	readonly defaultObjectGenerationMode = "tool";
	readonly modelId: OpenAIChatModelId;
	readonly settings: OpenAIChatSettings;
	private readonly config;
	constructor(
		modelId: OpenAIChatModelId,
		settings: OpenAIChatSettings,
		config: OpenAIChatConfig
	);
	get provider(): string;
	private getArgs;
	doGenerate(
		options: Parameters<LanguageModelV1["doGenerate"]>[0]
	): Promise<Awaited<ReturnType<LanguageModelV1["doGenerate"]>>>;
	doStream(
		options: Parameters<LanguageModelV1["doStream"]>[0]
	): Promise<Awaited<ReturnType<LanguageModelV1["doStream"]>>>;
}

type OpenAICompletionModelId = "gpt-3.5-turbo-instruct" | (string & {});
interface OpenAICompletionSettings {
	/**
	 * Echo back the prompt in addition to the completion
	 */
	echo?: boolean;
	/**
	 * Modify the likelihood of specified tokens appearing in the completion.
	 *
	 * Accepts a JSON object that maps tokens (specified by their token ID in
	 * the GPT tokenizer) to an associated bias value from -100 to 100. You
	 * can use this tokenizer tool to convert text to token IDs. Mathematically,
	 * the bias is added to the logits generated by the model prior to sampling.
	 * The exact effect will vary per model, but values between -1 and 1 should
	 * decrease or increase likelihood of selection; values like -100 or 100
	 * should result in a ban or exclusive selection of the relevant token.
	 *
	 * As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
	 * token from being generated.
	 */
	logitBias?: Record<number, number>;
	/**
	 * The suffix that comes after a completion of inserted text.
	 */
	suffix?: string;
	/**
	 * A unique identifier representing your end-user, which can help OpenAI to
	 * monitor and detect abuse. Learn more.
	 */
	user?: string;
}

type OpenAICompletionConfig = {
	provider: string;
	baseUrl: string;
	headers: () => Record<string, string | undefined>;
};
declare class OpenAICompletionLanguageModel implements LanguageModelV1 {
	readonly specificationVersion = "v1";
	readonly defaultObjectGenerationMode: undefined;
	readonly modelId: OpenAICompletionModelId;
	readonly settings: OpenAICompletionSettings;
	private readonly config;
	constructor(
		modelId: OpenAICompletionModelId,
		settings: OpenAICompletionSettings,
		config: OpenAICompletionConfig
	);
	get provider(): string;
	private getArgs;
	doGenerate(
		options: Parameters<LanguageModelV1["doGenerate"]>[0]
	): Promise<Awaited<ReturnType<LanguageModelV1["doGenerate"]>>>;
	doStream(
		options: Parameters<LanguageModelV1["doStream"]>[0]
	): Promise<Awaited<ReturnType<LanguageModelV1["doStream"]>>>;
}

/**
 * OpenAI provider.
 */
declare class OpenAI {
	readonly baseUrl?: string;
	readonly apiKey?: string;
	readonly organization?: string;
	constructor(options?: {
		baseUrl?: string;
		apiKey?: string;
		organization?: string;
	});
	private get baseConfig();
	chat(
		modelId: OpenAIChatModelId,
		settings?: OpenAIChatSettings
	): OpenAIChatLanguageModel;
	completion(
		modelId: OpenAICompletionModelId,
		settings?: OpenAICompletionSettings
	): OpenAICompletionLanguageModel;
}
/**
 * Default OpenAI provider instance.
 */
declare const openai: OpenAI;

export { OpenAI, openai };
```
